{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder sub-directories\n",
    "direct = os.getcwd()\n",
    "categories = ['business', 'entertainment', 'politics', 'sport', 'tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframe for articles\n",
    "dataframe = []\n",
    "\n",
    "for i, category in enumerate(categories):  # Loop for each sub-directory\n",
    "\n",
    "    curr_directory = os.path.join(direct, category)\n",
    "\n",
    "    for file in os.listdir(curr_directory):  # Loop for each article in category\n",
    "\n",
    "        with open(os.path.join(curr_directory, file)) as f:  \n",
    "            content = f.read()\n",
    "\n",
    "        curr_entry = {'Category': i, 'Content': content}  # Create dictionary for dataframe\n",
    "\n",
    "        dataframe.append(curr_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>4</td>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>4</td>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>4</td>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>4</td>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>4</td>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                            Content\n",
       "0            0  Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1            0  Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2            0  Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3            0  High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4            0  Pernod takeover talk lifts Domecq\\n\\nShares in...\n",
       "...        ...                                                ...\n",
       "2220         4  BT program to beat dialler scams\\n\\nBT is intr...\n",
       "2221         4  Spam e-mails tempt net shoppers\\n\\nComputer us...\n",
       "2222         4  Be careful how you code\\n\\nA new European dire...\n",
       "2223         4  US cyber security chief resigns\\n\\nThe man mak...\n",
       "2224         4  Losing yourself in online gaming\\n\\nOnline rol...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary from inputs\n",
    "df = pd.DataFrame(dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the input text for tfidf vectorization\n",
    "df['Content_Cleaned'] = df['Content'].str.replace('\\r', ' ')                                 # Remove line separators\n",
    "df['Content_Cleaned'] = df['Content_Cleaned'].str.replace('\\n', ' ')                         # Remove new line character\n",
    "df['Content_Cleaned'] = df['Content_Cleaned'].apply(lambda x: \" \".join(x.split()))           # Remove multiple spaces in a row\n",
    "df['Content_Cleaned'] = df['Content_Cleaned'].apply(lambda x: re.sub(r'\\[[0-9]*]', ' ', x))  # Remove in-text citations ([0], etc)\n",
    "\n",
    "df['Content_Cleaned'] = df['Content_Cleaned'].str.replace('\"', '')                           # Remove quotation marks\n",
    "df['Content_Cleaned'] = df['Content_Cleaned'].str.lower()                                    # Lowercase\n",
    "\n",
    "df['Content_Cleaned'] = df['Content_Cleaned'].str.replace(\"'s\", '')                          # Remove possessive form\n",
    "\n",
    "# Remove common punctuations\n",
    "punctuations = ['.,:;!?'] \n",
    "\n",
    "for punctuation in punctuations: \n",
    "    df['Content_Cleaned'] = df['Content_Cleaned'].str.replace('punctuation', '') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Stopwords\n",
    "stop_file = open('stopwords-en.txt', 'r')\n",
    "stop_words = stop_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(len(df)):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    curr_lemmatized = []\n",
    "    text = df.loc[row]['Content_Cleaned']\n",
    "    \n",
    "    text_words = text.split(\" \")  # Tokenize text\n",
    " \n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    " \n",
    "        if word not in stop_words: # Ignore stop-words\n",
    "\n",
    "            curr_lemmatized.append(lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Rejoin text \n",
    "    lemmatized_text = \" \".join(curr_lemmatized)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n",
    "\n",
    "df['Content_Lemmatized'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Lemmatized'], df['Category'], test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Selection\n",
    "ngram_range = (1, 2)  # Include unigram, bigrams\n",
    "min_df = 10\n",
    "max_df = 1.0\n",
    "max_features = 300 # Max 300 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1891, 300)\n",
      "(334, 300)\n"
     ]
    }
   ],
   "source": [
    "# Inititalize tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "# Train the tfidf vectorizer on training data\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "# Use vectorizer on testing data\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(random_state=0)\n",
    "\n",
    "# Possible Regularization Parameters\n",
    "C = [.0001, .001, .01, .1, 1]\n",
    "\n",
    "# Possible Gamma Parameters\n",
    "gamma = [.0001, .001, .01, .1, 1, 10, 100]\n",
    "\n",
    "# Possible Degree Parameters for Polynomial\n",
    "degree = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Possible Kernel Parameters\n",
    "kernel = ['linear', 'rbf', 'poly']\n",
    "\n",
    "probability = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SVC(random_state=0), n_iter=50,\n",
       "                   param_distributions={'C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                                        'degree': [1, 2, 3, 4, 5],\n",
       "                                        'gamma': [0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                                                  10, 100],\n",
       "                                        'kernel': ['linear', 'rbf', 'poly'],\n",
       "                                        'probability': [True]},\n",
       "                   random_state=0, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Perform Random Search\n",
    "random_grid = {'C': C,\n",
    "              'kernel': kernel,\n",
    "              'gamma': gamma,\n",
    "              'degree': degree,\n",
    "              'probability': probability\n",
    "             }\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=SVM,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=0)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'probability': True, 'kernel': 'poly', 'gamma': 100, 'degree': 3, 'C': 0.01}\n",
      "Accuracy: 0.9518795227194593\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 65 candidates, totalling 195 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=3, random_state=0, test_size=0.33, train_size=None),\n",
       "             estimator=SVC(random_state=0),\n",
       "             param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'kernel': ['linear'], 'probability': [True]},\n",
       "                         {'C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'degree': [1, 2, 3, 4, 5], 'kernel': ['poly'],\n",
       "                          'probability': [True]},\n",
       "                         {'C': [0.0001, 0.001, 0.01, 0.1, 1],\n",
       "                          'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                          'kernel': ['rbf'], 'probability': [True]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Use results from random search to define grid search\n",
    "param_grid = [\n",
    "  {'C': C, 'kernel':['linear'], 'probability': probability},\n",
    "  {'C': C, 'kernel':['poly'], 'degree': degree, 'probability': probability},\n",
    "  {'C': C, 'kernel':['rbf'], 'gamma': gamma, 'probability': probability}\n",
    "]\n",
    "\n",
    "# Create a base model\n",
    "svc = svm.SVC(random_state=0)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=svc, \n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=ShuffleSplit(n_splits = 3, test_size = .33, random_state = 0),\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'degree': 2, 'kernel': 'poly', 'probability': True}\n",
      "Accuracy: 0.9557333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, degree=2, kernel='poly', probability=True, random_state=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use grid search results\n",
    "best_SVM = grid_search.best_estimator_\n",
    "best_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the predict\n",
    "best_SVM.fit(features_train, labels_train)\n",
    "svc_pred = best_SVM.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Training accuracy\n",
    "print(\"The training accuracy for model is: \", accuracy_score(labels_train, best_SVM.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is:  0.9550898203592815\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy for model is: \", accuracy_score(labels_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the models \n",
    "import pickle\n",
    "\n",
    "with open('tfidf', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)\n",
    "    \n",
    "with open('SVM', 'wb') as output:\n",
    "    pickle.dump(best_SVM, output)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b31e9a99c67e264b757e0be9f27de848ee6a7e59a2fc5b072d4c22395ff19392"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
